{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "LR = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def FloatTensor(x):\n",
    "    return torch.tensor(x, device=device, dtype=torch.float)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "        self.sm = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.sm(self.l2(x))\n",
    "        return x\n",
    "    \n",
    "    def get_act_logp(self, x):\n",
    "        #with torch.no_grad():\n",
    "        act_p = self.forward(x)\n",
    "        m = torch.distributions.Categorical(act_p)\n",
    "        act = m.sample()\n",
    "        logp = m.log_prob(act)\n",
    "        return act, logp\n",
    "\n",
    "    def act(self, x):\n",
    "        with torch.no_grad():\n",
    "            act, _ = self.get_act_logp(x)\n",
    "            return act\n",
    "\n",
    "model = Network().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    act, logp = model.get_act_logp(state)\n",
    "    return act, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "PLOT_MEAN = 10\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    \n",
    "    if len(durations_t) >= PLOT_MEAN:\n",
    "        means = durations_t.unfold(0, PLOT_MEAN, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(PLOT_MEAN-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "\n",
    "def Qsa(episode_rewards):\n",
    "    return [np.sum([gamma ** i * r for i, r in enumerate(episode_rewards[j:])]) for j in range(len(episode_rewards))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(episode = 50):\n",
    "    plt.ion()\n",
    "    \n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        state = FloatTensor([state])\n",
    "\n",
    "        logps = []\n",
    "        rewards = []\n",
    "\n",
    "        for step in count(): \n",
    "            #env.render()\n",
    "            action, logp = select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state = FloatTensor([next_state])\n",
    "            rewards.append(reward)\n",
    "            logps.append(logp)\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                Qsas = Qsa(rewards)\n",
    "                # crossentropy : Ï€(s,a) * Q(s, a)\n",
    "                QsaLogps = [logps[i] * -1. * Qsas[i] for i in range(step)]\n",
    "                policy_loss = torch.cat(QsaLogps).mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                plot_durations()\n",
    "                episode_durations.append(step)\n",
    "                break\n",
    "S\n",
    "        if e % 50 == 0:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\".format(e, step, '\\033[92m' if step >= 195 else '\\033[0m'))\n",
    "    \n",
    "    print('Complete')\n",
    "    #env.render()\n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: TkAgg\n\u001b[0m Episode 0 finished after 34 steps\n\u001b[0m Episode 10 finished after 18 steps\n\u001b[0m Episode 20 finished after 24 steps\n\u001b[0m Episode 30 finished after 36 steps\n\u001b[0m Episode 40 finished after 11 steps\n\u001b[0m Episode 50 finished after 21 steps\n\u001b[0m Episode 60 finished after 15 steps\n\u001b[0m Episode 70 finished after 18 steps\n\u001b[0m Episode 80 finished after 13 steps\n\u001b[0m Episode 90 finished after 21 steps\n\u001b[0m Episode 100 finished after 21 steps\n\u001b[0m Episode 110 finished after 13 steps\n\u001b[0m Episode 120 finished after 32 steps\n\u001b[0m Episode 130 finished after 17 steps\n\u001b[0m Episode 140 finished after 20 steps\n\u001b[0m Episode 150 finished after 17 steps\n\u001b[0m Episode 160 finished after 43 steps\n\u001b[0m Episode 170 finished after 95 steps\n\u001b[0m Episode 180 finished after 25 steps\n\u001b[0m Episode 190 finished after 16 steps\n\u001b[0m Episode 200 finished after 19 steps\n\u001b[0m Episode 210 finished after 68 steps\n\u001b[0m Episode 220 finished after 11 steps\n\u001b[0m Episode 230 finished after 36 steps\n\u001b[0m Episode 240 finished after 39 steps\n\u001b[0m Episode 250 finished after 44 steps\n\u001b[0m Episode 260 finished after 27 steps\n\u001b[0m Episode 270 finished after 55 steps\n\u001b[0m Episode 280 finished after 40 steps\n\u001b[0m Episode 290 finished after 139 steps\n\u001b[0m Episode 300 finished after 24 steps\n\u001b[0m Episode 310 finished after 69 steps\n\u001b[0m Episode 320 finished after 68 steps\n\u001b[0m Episode 330 finished after 48 steps\n\u001b[0m Episode 340 finished after 49 steps\n\u001b[0m Episode 350 finished after 130 steps\n\u001b[0m Episode 360 finished after 33 steps\n\u001b[0m Episode 370 finished after 90 steps\n\u001b[0m Episode 380 finished after 136 steps\n\u001b[0m Episode 390 finished after 79 steps\n\u001b[0m Episode 400 finished after 143 steps\n\u001b[92m Episode 410 finished after 199 steps\n\u001b[92m Episode 420 finished after 199 steps\n\u001b[92m Episode 430 finished after 199 steps\n\u001b[0m Episode 440 finished after 177 steps\n\u001b[0m Episode 450 finished after 106 steps\n\u001b[92m Episode 460 finished after 199 steps\n\u001b[0m Episode 470 finished after 120 steps\n\u001b[0m Episode 480 finished after 119 steps\n\u001b[0m Episode 490 finished after 181 steps\nComplete\n"
    }
   ],
   "source": [
    "%matplotlib\n",
    "run_episode(500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "100 times, average :  184.33 steps\n"
    }
   ],
   "source": [
    "steps = []\n",
    "for t in range(100):\n",
    "\n",
    "    obs = env.reset()\n",
    "    obs = FloatTensor([obs])\n",
    "    for step in range(300):\n",
    "        act = model.act(obs)\n",
    "        nobs, r, done, _= env.step(act.item())\n",
    "\n",
    "        obs = FloatTensor([nobs])\n",
    "\n",
    "        if done:\n",
    "            #print('total %d steps' % step)\n",
    "            steps.append(step)\n",
    "            break\n",
    "print(\"100 times, average : \", np.average(steps), \"steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1595299342634"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}