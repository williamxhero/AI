{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#循环写入的数组\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # cap : 10000\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, trans):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = trans\n",
    "        #loop [0 : cap)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        #从A中随机抽取B个. B > A 时抛异常\n",
    "        return random.sample(self.memory, batch_size) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = Network().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return model(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "PLOT_MEAN = 10\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    \n",
    "    if len(durations_t) >= PLOT_MEAN:\n",
    "        means = durations_t.unfold(0, PLOT_MEAN, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(PLOT_MEAN-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    BATCH_SIZE = 128  # Q-learning batch size\n",
    "    GAMMA = 0.7\n",
    "    ALPHA = 0.7\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    trans = memory.sample(BATCH_SIZE)\n",
    "    batch = list(zip(*trans))\n",
    "    \n",
    "    batch_state = torch.cat(batch[0])\n",
    "    batch_action = torch.cat(batch[1])\n",
    "    batch_nstate = torch.cat(batch[2])\n",
    "    batch_reward = torch.cat(batch[3])\n",
    "  \n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    max_next_q_values = model(batch_nstate).detach().max(1)[0]\n",
    "    \n",
    "    expected_q_values = batch_reward + (max_next_q_values * GAMMA)\n",
    "    expected_q_values = expected_q_values.unsqueeze(1)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values, expected_q_values)\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 50  # number of episodes\n",
    "\n",
    "def run_episode():\n",
    "    plt.ion()\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "\n",
    "        for t in count(): \n",
    "            #env.render()\n",
    "            action = select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            reward = -200 if done else reward\n",
    "\n",
    "            next_state = torch.tensor([next_state], device=device, dtype=torch.float)\n",
    "            reward = torch.tensor([reward], device=device, dtype=torch.float)\n",
    "            \n",
    "            memory.push((state, action, next_state, reward))\n",
    "            state = next_state\n",
    "\n",
    "            learn()\n",
    "\n",
    "            if done:\n",
    "                print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                      .format(e, t, '\\033[92m' if t >= 195 else '\\033[0m'))\n",
    "                episode_durations.append(t)\n",
    "                plot_durations()\n",
    "                break\n",
    "                \n",
    "    print('Complete')\n",
    "    #env.render()\n",
    "    env.close()\n",
    "    plt.ioff()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using matplotlib backend: TkAgg\n\u001b[0m Episode 0 finished after 13 steps\n\u001b[0m Episode 1 finished after 11 steps\n\u001b[0m Episode 2 finished after 9 steps\n\u001b[0m Episode 3 finished after 11 steps\n\u001b[0m Episode 4 finished after 11 steps\n\u001b[0m Episode 5 finished after 13 steps\n\u001b[0m Episode 6 finished after 13 steps\n\u001b[0m Episode 7 finished after 9 steps\n\u001b[0m Episode 8 finished after 11 steps\n\u001b[0m Episode 9 finished after 10 steps\n\u001b[0m Episode 10 finished after 13 steps\n\u001b[0m Episode 11 finished after 11 steps\n\u001b[0m Episode 12 finished after 8 steps\n\u001b[0m Episode 13 finished after 9 steps\n\u001b[0m Episode 14 finished after 10 steps\n\u001b[0m Episode 15 finished after 10 steps\n\u001b[0m Episode 16 finished after 9 steps\n\u001b[0m Episode 17 finished after 9 steps\n\u001b[0m Episode 18 finished after 10 steps\n\u001b[0m Episode 19 finished after 9 steps\n\u001b[0m Episode 20 finished after 8 steps\n\u001b[0m Episode 21 finished after 9 steps\n\u001b[0m Episode 22 finished after 9 steps\n\u001b[0m Episode 23 finished after 13 steps\n\u001b[0m Episode 24 finished after 15 steps\n\u001b[0m Episode 25 finished after 11 steps\n\u001b[0m Episode 26 finished after 14 steps\n\u001b[0m Episode 27 finished after 13 steps\n\u001b[0m Episode 28 finished after 14 steps\n\u001b[0m Episode 29 finished after 19 steps\n\u001b[0m Episode 30 finished after 12 steps\n\u001b[0m Episode 31 finished after 13 steps\n\u001b[0m Episode 32 finished after 18 steps\n\u001b[0m Episode 33 finished after 16 steps\n\u001b[0m Episode 34 finished after 15 steps\n\u001b[0m Episode 35 finished after 24 steps\n\u001b[0m Episode 36 finished after 11 steps\n\u001b[0m Episode 37 finished after 69 steps\n\u001b[0m Episode 38 finished after 25 steps\n\u001b[0m Episode 39 finished after 20 steps\n\u001b[0m Episode 40 finished after 22 steps\n\u001b[0m Episode 41 finished after 18 steps\n\u001b[0m Episode 42 finished after 17 steps\n\u001b[0m Episode 43 finished after 14 steps\n\u001b[0m Episode 44 finished after 17 steps\n\u001b[0m Episode 45 finished after 22 steps\n\u001b[0m Episode 46 finished after 20 steps\n\u001b[0m Episode 47 finished after 140 steps\n\u001b[92m Episode 48 finished after 199 steps\n\u001b[92m Episode 49 finished after 199 steps\n\u001b[92m Episode 50 finished after 199 steps\n\u001b[92m Episode 51 finished after 199 steps\n\u001b[92m Episode 52 finished after 199 steps\n\u001b[92m Episode 53 finished after 199 steps\n\u001b[92m Episode 54 finished after 199 steps\n\u001b[92m Episode 55 finished after 199 steps\n\u001b[92m Episode 56 finished after 199 steps\n\u001b[92m Episode 57 finished after 199 steps\n\u001b[92m Episode 58 finished after 199 steps\n\u001b[92m Episode 59 finished after 199 steps\n\u001b[92m Episode 60 finished after 199 steps\n\u001b[92m Episode 61 finished after 199 steps\n\u001b[92m Episode 62 finished after 199 steps\n\u001b[92m Episode 63 finished after 199 steps\n\u001b[92m Episode 64 finished after 198 steps\n\u001b[92m Episode 65 finished after 199 steps\n\u001b[92m Episode 66 finished after 199 steps\n\u001b[92m Episode 67 finished after 199 steps\n\u001b[92m Episode 68 finished after 199 steps\n\u001b[92m Episode 69 finished after 199 steps\n\u001b[92m Episode 70 finished after 199 steps\n\u001b[92m Episode 71 finished after 199 steps\n\u001b[92m Episode 72 finished after 199 steps\n\u001b[92m Episode 73 finished after 199 steps\n\u001b[92m Episode 74 finished after 199 steps\n\u001b[92m Episode 75 finished after 199 steps\n\u001b[92m Episode 76 finished after 199 steps\n\u001b[92m Episode 77 finished after 199 steps\n\u001b[92m Episode 78 finished after 199 steps\n\u001b[92m Episode 79 finished after 199 steps\n\u001b[92m Episode 80 finished after 199 steps\n\u001b[92m Episode 81 finished after 199 steps\n\u001b[92m Episode 82 finished after 199 steps\n\u001b[92m Episode 83 finished after 199 steps\n\u001b[92m Episode 84 finished after 199 steps\n\u001b[92m Episode 85 finished after 199 steps\n\u001b[92m Episode 86 finished after 199 steps\n\u001b[92m Episode 87 finished after 199 steps\n\u001b[0m Episode 88 finished after 78 steps\n\u001b[92m Episode 89 finished after 199 steps\n\u001b[92m Episode 90 finished after 199 steps\n\u001b[92m Episode 91 finished after 199 steps\n\u001b[0m Episode 92 finished after 140 steps\n\u001b[92m Episode 93 finished after 199 steps\n\u001b[92m Episode 94 finished after 199 steps\n\u001b[92m Episode 95 finished after 199 steps\n\u001b[92m Episode 96 finished after 199 steps\n\u001b[92m Episode 97 finished after 199 steps\n\u001b[92m Episode 98 finished after 199 steps\n\u001b[92m Episode 99 finished after 199 steps\nComplete\n"
    }
   ],
   "source": [
    "%matplotlib\n",
    "run_episode()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'FloatTensor' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4a9dcbae8d83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m               \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FloatTensor' is not defined"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "rewards = []\n",
    "for trails in range(100):\n",
    "    state = env.reset()\n",
    "    reward = 0\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        with torch.no_grad():\n",
    "              action = model(torch.tensor([state], device=device, dtype=torch.float)).max(1)[1].view(1, 1)\n",
    "        state, r, done, _ = env.step(action.item())\n",
    "        reward += r\n",
    "        if done:\n",
    "            rewards.append(reward)\n",
    "            print(reward)\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "print(\"rewards:\", np.average(rewards))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1595236621361"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}